# üìä √âtape 8 : Fonction d'√©valuation

## üéØ Objectif de la fonction d'√©valuation

La fonction d'√©valuation permet de mesurer les performances du mod√®le sur des donn√©es non vues pendant l'entra√Ænement, sans modifier les param√®tres du mod√®le.

## üíª Code de la fonction d'√©valuation

```python
# Fonction d'√©valuation
def evaluate_model(dataloader, model, loss_fn):
    model.eval()
    total_loss = 0
# D√©sactivation du calcul des gradients
    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            predictions = model(X_batch)
            loss = loss_fn(predictions, y_batch)
            total_loss += loss.item()
    return total_loss / len(dataloader)
```

## üîç Explication d√©taill√©e

### 1. Mode √âvaluation 

`model.eval()`

- D√©sactive les couches sp√©cifiques d'entra√Ænement (dropout, batch normalization)
- Pr√©pare le mod√®le pour l'inf√©rence
- Assure une √©valuation coh√©rente

### 2. D√©sactivation du calcul des gradients

`with torch.no_grad():`

- √âconomise de la m√©moire
- Acc√©l√®re le processus d'√©valuation
- Emp√™che toute modification des param√®tres

### 3. Calcul de la perte
```python
   predictions = model(X_batch)
   loss = loss_fn(predictions, y_batch)
```
- Propage les donn√©es √† travers le mod√®le
- Calcule l'erreur sans r√©tropropagation

### 4. Accumulation de la perte

`total_loss += loss.item()`

- Accumule la perte pour chaque lot
- Permet de calculer la perte moyenne

### 5. Retour de la perte moyenne

`return total_loss / len(dataloader)`

- Calcule la perte moyenne sur tous les lots

## ‚ö†Ô∏è Points d'attention

- **Coh√©rence** : Toujours utiliser `model.eval()`
- **No Gradient** : Essentiel pour l'efficacit√©
- **M√©triques vari√©es** : Compl√©ter la perte par d'autres indicateurs

## üöÄ Prochaines √©tapes

Avec notre fonction d'√©valuation d√©finie, nous sommes pr√™ts √† passer √† l'entra√Ænement complet du mod√®le et √† l'analyse de ses performances.






















